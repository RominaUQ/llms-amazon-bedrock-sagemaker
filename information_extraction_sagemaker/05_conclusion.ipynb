{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results and Performance Comparisons\n",
    "\n",
    "## Mistral-7b (Fine-Tuned)\n",
    "- Average fine-tuning duration on an ml.g5.24xlarge instance: 12 hours for 5 epochs.\n",
    "- Sequential inference on a 2xlarge instance for the test dataset: 10 minutes and 33 seconds.\n",
    "- Improved performance with more epochs: 98% accuracy for 5 epochs compared to 92% for 1 epoch.\n",
    "- Given Mistral-7b's nature as a text generation model, parsing its output to extract intent can be challenging due to tendencies for character repetition and generation of additional characters.\n",
    "\n",
    "## Flan-T5-XL (Fine-Tuned)\n",
    "- Average fine-tuning duration on an ml.g5.24xlarge instance: 43 minutes for 1 epoch, 2.5 hours for 5 epochs.\n",
    "- Evaluation on a 2xlarge instance took approximately 7 minutes (synchronous).\n",
    "- Marginal improvement in accuracy with increased epochs: from 97.5% (1 epoch) to 98.46% (5 epochs).\n",
    "\n",
    "## Llama2-70b-chat\n",
    "- Ineffective in identifying correct intents in ambiguous scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results \n",
    "\n",
    "Each model underwent fine-tuning and evaluation multiple times, and the average accuracy is reported here.\n",
    "\n",
    "| Model                                          | Accuracy  | Fine-tuning Duration (mins) |\n",
    "|------------------------------------------------|-----------|-----------------------------|\n",
    "| Mistral-7b (fine-tuned 5 epochs)               | 98.97%    | 720                         |\n",
    "| Flan-T5-XL (fine-tuned 5 epochs)               | 98.46%    | 150                         |\n",
    "| Llama2-70b-chat (one-shot, classes in prompt)  | 77.52%    | N/A                         |\n",
    "| Llama2-70b-chat (zero-shot, no classes prompt) | 10.85%    | N/A                         |\n",
    "| Flan-T5-XL (base model)\t                     | 0.0%  \t | N/A                         |\n",
    "| Mistral-7b (base model)                        | 0.0%  \t | N/A                         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the break down of each model's performance on the test dataset by intent.\n",
    "\n",
    "| Main Intent          | Sub Intent           | Example Count | Llama2-70b Zero-shot Accuracy | Llama2-70b Single-shot Accuracy | Flant5-xl Fine-tuned Accuracy | Mistral-7b Fine-tuned Accuracy |\n",
    "|----------------------|----------------------|-------|-------------------------------|---------------------------------|-------------------------------|--------------------------------|\n",
    "| Customer Retention   | Complaint            | 63    | 7.94%                         | 11.11%                          | 98.41%                        | 98.41%                         |\n",
    "| Customer Retention   | Escalation           | 49    | 91.84%                        | 100.00%                         | 100.00%                       | 100.00%                        |\n",
    "| Customer Retention   | Free Product Upgrade | 50    | 0.00%                         | 66.00%                          | 100.00%                       | 100.00%                        |\n",
    "| Health Cover         | Add Extras           | 38    | 0.00%                         | 100.00%                         | 97.37%                        | 100.00%                        |\n",
    "| Health Cover         | Add Hospital         | 44    | 0.00%                         | 84.09%                          | 100.00%                       | 97.73%                         |\n",
    "| Health Cover         | Cancel Policy        | 43    | 0.00%                         | 100.00%                         | 100.00%                       | 97.67%                         |\n",
    "| Health Cover         | New Policy           | 41    | 0.00%                         | 92.68%                          | 100.00%                       | 100.00%                        |\n",
    "| Health Cover         | Remove Extras        | 47    | 0.00%                         | 91.49%                          | 100.00%                       | 100.00%                        |\n",
    "| Health Cover         | Remove Hospital      | 53    | 0.00%                         | 88.68%                          | 100.00%                       | 100.00%                        |\n",
    "| Life Cover           | Beneficiary Info     | 45    | 0.00%                         | 97.78%                          | 97.78%                        | 97.78%                         |\n",
    "| Life Cover           | Cancel Policy        | 47    | 0.00%                         | 76.60%                          | 100.00%                       | 100.00%                        |\n",
    "| Life Cover           | New Policy           | 40    | 0.00%                         | 97.50%                          | 92.50%                        | 100.00%                        |\n",
    "| Profile Update       | Contact Info         | 45    | 35.56%                        | 95.56%                          | 95.56%                        | 95.56%                         |\n",
    "| Profile Update       | Members              | 52    | 0.00%                         | 15.38%                          | 98.08%                        | 98.08%                         |\n",
    "| Profile Update       | Payment Info         | 47    | 40.43%                        | 97.87%                          | 100.00%                       | 100.00%                        |\n",
    "| Technical Support    | Login Issues         | 39    | 0.00%                         | 89.74%                          | 97.44%                        | 100.00%                        |\n",
    "| Technical Support    | Portal Navigation    | 40    | 0.00%                         | 52.50%                          | 95.00%                        | 97.50%                         |\n",
    "\n",
    "This comparative analysis reveals the trade-offs between fine-tuning duration and model accuracy, highlighting the effectiveness of fine-tuning, models like Mistral-7b and FlanT5-XL, in improving classification accuracy. It also demonstrates the performance boost of smaller models compared to larger models once fine-tuned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "1. Delete the Amazon SageMaker endpoints, configuration and models.\n",
    "2. Delete the Amazon S3 bucket created for this example.\n",
    "3. Delete the Amazon SageMaker notebook instance (if you used one to run this example).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Large Language Models have revolutionized information extraction from unstructured text data. These models excel in tasks such as classifying information and extracting key entities from various documents, achieving state-of-the-art results with minimal data. SageMaker JumpStart significantly simplifies the process of fine-tuning and deploying LLMs, making it easier for developers to incorporate advanced natural language processing into their applications.\n",
    "\n",
    "In this example, we showed how LLMs can be leveraged for information extraction through prompt engineering and fine-tuning. Prompt engineering involves crafting specific prompts that guide LLMs to generate useful responses for tasks like sensitive information detection, entity extraction, and text classification. Despite its effectiveness, prompt engineering has limitations, especially when dealing with complex tasks or large set of classes. In such cases, fine-tuning LLMs, even smaller models, on domain-specific data can significantly enhance performance.\n",
    "\n",
    "For example, the sensitive data extraction task illustrates how specific prompts can guide LLMs to redact personal information accurately. However, for more nuanced tasks like intent classification in customer interactions, where the context and intent are not always clear, fine-tuning becomes essential. This example provides practical examples of both techniques, showing how fine-tuning can outperform prompt engineering in complex scenarios, even with smaller and more cost effective models. \n",
    "\n",
    "In summary, while prompt engineering is a good starting point for simpler use cases, fine-tuning offers a more robust solution for complex information extraction tasks, ensuring higher accuracy and adaptability to specific use cases. SageMaker JumpStart's tools and services facilitate this process, making it accessible for individuals and teams across all levels of machine learning expertise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
