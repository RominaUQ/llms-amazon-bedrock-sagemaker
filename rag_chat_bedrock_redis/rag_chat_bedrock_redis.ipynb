{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a27eb455-3769-4973-8d8d-606954148749",
   "metadata": {},
   "source": [
    "# RAG-based coversation using Amazon BedRock, SageMaker and Redis\n",
    "This example demonstrates how to create a RAG-based conversation system using Amazon Bedrock, SageMaker, and Redis. The steps involved are:\n",
    "\n",
    "- Setting up the environment and installing necessary packages.\n",
    "- Loading a document from the AWS Machine Learning Blog.\n",
    "- Splitting the document into smaller chunks.\n",
    "- Connecting to the Bedrock Embedding endpoint.\n",
    "- Using Redis as a vector store to store the document embeddings.\n",
    "- Performing a similarity search in the vector store.\n",
    "- Creating a chat application using the Bedrock model, Redis vector store, and Langchain library.\n",
    "- Using the chat application to answer questions based on the loaded document.\n",
    "- Using Redis as a memory store for the conversation buffer memory to retain context and provide a conversational experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49d29f-ece1-492a-814e-ff87f2aa7a38",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86c905-7913-4697-ba96-0635d2fe3a91",
   "metadata": {},
   "source": [
    "Install necessary packages bedrock sdand set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17da300c-f7fc-4a85-b6c2-52bd53e6bce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet redis langchain pypdf pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a73cd2-0d63-498b-980a-21d503b36153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Disable warnings and verbose logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import boto3\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "bedrock = boto3.client(\"bedrock\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f0c0c",
   "metadata": {},
   "source": [
    "Getting the Foundation Models from Bedrock. The foundation models are the models that are available for use in Bedrock. The list of foundation models can be obtained using the `list_foundation_models()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4f8039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'b7fa5f65-f1f6-436e-801b-d4785569194e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 15 Oct 2023 07:10:19 GMT', 'content-type': 'application/json', 'content-length': '373', 'connection': 'keep-alive', 'x-amzn-requestid': 'b7fa5f65-f1f6-436e-801b-d4785569194e'}, 'RetryAttempts': 0}, 'modelDetails': {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-g1-text-02', 'modelId': 'amazon.titan-embed-g1-text-02', 'modelName': 'Titan Text Embeddings v2', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['EMBEDDING'], 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND']}}\n"
     ]
    }
   ],
   "source": [
    "response = bedrock.list_foundation_models()\n",
    "model = bedrock.get_foundation_model(\n",
    "    modelIdentifier=\"amazon.titan-embed-g1-text-02\"\n",
    ")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f07a46",
   "metadata": {},
   "source": [
    "In order to invoke the model with our prompts, we need to create a Bedrock Runtime. Bedrock Runtime currently supports `InvokeModel` and `InvokeModelWithResponseStream` actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73cc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\", region_name=region\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71060f2-9f05-4c27-a861-a88472bba25a",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "For this example we use one of the AWS Machine Learnign Blog posts, [Announcing New Tools to Help Every Business Embrace Generative AI - by Swami Sivasubramanian](https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-to-help-every-business-embrace-generative-ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59fa7c0-19c6-467d-949c-2f19bb26ea0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_file = \"data/announcing_new_tools_to_help_every_business_embrace_generative_ai.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_file)\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d05e5",
   "metadata": {},
   "source": [
    "We could see the PyPDFLoader, loaded the file and split it into 9 documents, one page per document. Now, let's see the print out the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40bfea0f-fb66-4937-8200-df3467bc2f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Machine Learning BlogAnnouncing New Tools to Help Every Business EmbraceGenerative AIby Swami Sivasubramanian | on 28 SEP 2023 | in Announcements, Artiﬁcial Intelligence, Generative AI |Permalink |  Comments |  ShareFrom startups to enterprises, organizations of all sizes are getting started with generative AI. They want tocapitalize on generative AI and translate the momentum from betas, prototypes, and demos into real-worldproductivity gains and innovations. But what do organizations need to bring generative AI into the enterprise andmake it real? When we talk to customers, they tell us they need security and privacy, scale and price-performance, and most importantly tech that is relevant to their business. We are excited to announce newcapabilities and services today to allow organizations big and small to use generative AI in creative ways, buildingnew applications and improving how they work. At AWS, we are hyper-focused on helping our customers in a fewways:• Making it easy to build generative AI applications with security and privacy built in• Focusing on the most performant, low cost infrastructure for generative AI so you can train your own modelsand run inference at scale• Providing generative AI-powered applications for the enterprise to transform how work gets done• Enabling data as your diﬀerentiator to customize foundation models (FMs) and make them an expert on yourbusiness, your data, and your companyTo help a broad range of organizations build diﬀerentiated generative AI experiences, AWS has been workinghand-in-hand with our customers, including BBVA, Thomson Reuters, United Airlines, Philips, and LexisNexisLegal & Professional. And with the new capabilities launched today, we look forward to enhanced productivity,improved customer engagement, and more personalized experiences that will transform how companies getwork done.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a07647-7028-4e8d-9b93-5c247dfd57a3",
   "metadata": {},
   "source": [
    "Splitting the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b27865-d927-433b-b675-41f35221cdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, chunk_overlap=150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(len(splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9306205d-b40d-4d8e-8424-92f756eb1496",
   "metadata": {},
   "source": [
    "Connect to Bedrock Embedding endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f73117d-426d-455e-9a47-92cee6c4674b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embeddings = BedrockEmbeddings(client=bedrock_runtime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79692e89-8bb4-42db-a62f-8046fc764710",
   "metadata": {},
   "source": [
    "## Redis as Vector Store\n",
    "In this example, we use Redis as the vector store. Redis has a [vector similarity search](https://redis.io/docs/interact/search-and-query/search/vectors/) capability which makes it an ideal choice for both development and production.\n",
    "\n",
    "### Redis Deployment Patterns\n",
    "\n",
    "- EC2: In this approach, we simply deploy the Redis stack in EC2 and, depending on the workload, utilize persistence and clustering features.\n",
    "- ECS or EKS: In this approach, we can deploy the Redis stack container simply to a Fargate cluster, which is a cost-effective and scalable approach. The workshop, [Solving Data Challenges in Cloud Applications with Redis](https://redislabs.awsworkshop.io/) is a good, well-rounded set of examples on utilizing Redis features using ECS and Fargate.\n",
    "- [Redis Cloud](https://redis.com/redis-enterprise-cloud/overview/)\n",
    "- Redis on local Docker: This is an easy approach for development and non-production environments. This can be achieved by running the following command:\n",
    "    ```bash\n",
    "    # Remove the Redis container and recreate a new one.\n",
    "    !docker container rm -f redis-stack\n",
    "    !docker run -d --name redis-stack -p 6379:6379 redis/redis-stack\n",
    "    ``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcf3bf-7324-4ef7-9c2e-5527e6592759",
   "metadata": {},
   "source": [
    "### Store Embeddings\n",
    "In the following we create embeddings for the splits and store them in the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc89b5e1-ac86-4510-9c3d-b728e412b8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.redis import Redis\n",
    "\n",
    "redis_url = \"redis://redis:6379\"\n",
    "index_name = \"doc_index\"\n",
    "\n",
    "vectordb = Redis.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    redis_url=redis_url,\n",
    "    index_name=index_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b1dc5",
   "metadata": {},
   "source": [
    "Now let's test our vector store by performing a similarity search. We will ask a question and ask for the top 3 (`k=3`) most similar documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b38bc64-e48a-4abb-a43b-f0fb8c9e7b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings supports more than 25 languages and a context length of upto 8,192 tokens, making it well suited to work with single words, phrases, or entire documents based on thecustomer’s use case. The model returns output vectors of 1,536 dimensions, giving it a high degree of accuracy,while also optimizing for low-latency, cost-eﬀective results. With new models and capabilities, it’s easy to useyour organization’s data as a strategic asset to customize foundation models and build more diﬀerentiatedexperiences.Third, because the data customers want to use for customization is such valuable IP, they need it to remain secureand private. With security and privacy built in since day one, Amazon Bedrock customers can trust that their dataremains protected. None of the customer’s data is used to train the original base FMs. All data is encrypted at restand in transit. And you can expect the same AWS access controls that you have with any other AWS service.Today, we are excited to build on this foundation and introduce new security and governance capabilities –Amazon Bedrock is now a HIPAA eligible service and can be used in compliance with GDPR, allowing even morecustomers to beneﬁt from generative AI. New governance capabilities include integration with AmazonCloudWatch to track usage metrics and build customized dashboards and integration with AWS CloudTrail tomonitor API activity and troubleshoot issues. These new governance and security capabilities help organizationsunlock\n",
      "----------------------------------------------------------------------------------------------------\n",
      "powerful, general purpose capabilities built to support a variety of use cases. The ﬁrst of these models generallyavailable to customers, Amazon Titan Embeddings, is an LLM that converts text into numerical representations(known as embeddings) to power RAG use cases. FMs are well suited for a wide variety of tasks, but they can onlyrespond to questions based on learnings from the training data and contextual information in a prompt, limitingtheir eﬀectiveness when responses require timely knowledge or proprietary data. Data is the diﬀerence betweena general generative AI application and one that truly knows your business and your customer. To augment FMresponses with additional data, many organizations turn to RAG, a popular model-customization technique wherean FM connects to a knowledge source that it can reference to augment its responses. To get started with RAG,customers ﬁrst need access to an embedding model to convert their data into vectors that allow the FM to moreeasily understand the semantic meaning and relationships between data. Building an embeddings model requiresmassive amounts of data, resources, and ML expertise, putting RAG out of reach for many organizations. AmazonTitan Embeddings makes it easier for customers to get started with RAG to extend the power of any FM usingtheir proprietary data. Amazon Titan Embeddings supports more than 25 languages and a context length of upto 8,192 tokens, making it well suited to work with single words, phrases, or\n",
      "----------------------------------------------------------------------------------------------------\n",
      "consistent user experience even during peak traﬃc timesWith the general availability of Amazon Bedrock, more customers will have access to Bedrock’s comprehensivecapabilities. Customers can easily experiment with a variety of top FMs, customize them privately with their datausing techniques such as ﬁne tuning and RAG, and create managed agents that execute complex business tasks—from booking travel and processing insurance claims to creating ad campaigns and managing inventory—allwithout writing any code. Since Amazon Bedrock is serverless, customers don’t have to manage anyinfrastructure, and they can securely integrate and deploy generative AI capabilities into their applications usingthe AWS services they are already familiar with.Second, model choice has been a cornerstone of what makes Amazon Bedrock a unique, diﬀerentiated service forour customers. This early in the adoption of generative AI, there is no single model that unlocks all the value ofgenerative AI, and customers need the ability to work with a range of high-performing models. We are excited toannounce the general availability of Amazon Titan Embeddings and coming in the next few weeks availability ofLlama 2, Meta’s next generation large language model (LLM) – joining existing model providers AI21 Labs,Anthropic, Cohere, Stability AI, and Amazon in further expanding choice and ﬂexibility for customers. AmazonBedrock is the ﬁrst fully managed generative AI service to oﬀer Llama 2, Meta’s next-generation LLM,\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"How many languages are supported for embeddings?\"\n",
    "\n",
    "result = vectordb.similarity_search(question, k=3)\n",
    "\n",
    "for r in result:\n",
    "    print(r.page_content)\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3c359-6215-4336-999c-a2c5624fad58",
   "metadata": {},
   "source": [
    "## Chat Application\n",
    "In this section, we will create a chat application using the Bedrock model, Redis vector store, and Langchain library. We'll use the conversation buffer memory and the retrieval chain. The chat application will be able to answer questions based on the document we loaded earlier. \n",
    "\n",
    "We will also use Redis as our memory store for the conversation buffer memory. This will retain the context and provide a conversational experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0571a569-2716-4b33-ab0f-02656c4383f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260f88fb-394a-4058-8a32-77eee5c6c716\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
    "import uuid\n",
    "\n",
    "# Generate a random session id\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "message_history = RedisChatMessageHistory(url=redis_url, session_id=session_id)\n",
    "print(session_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d448b84e-8927-461e-a034-f18320aca815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    chat_memory=message_history,\n",
    "    return_messages=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8df798ad-4cc5-4dac-8bee-60cb583a4eed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=bedrock_runtime)\n",
    "\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm, retriever=vectordb.as_retriever(), memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7eeda",
   "metadata": {},
   "source": [
    "### Start the conversation\n",
    "We start our conversation by asking a question. The question is passed to the conversation buffer memory, which returns the most relevant document. The document is then passed to the retrieval chain, which returns the most relevant answer. The answer is then returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54b8894-65ab-4aef-bafe-1033453c8622",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context provided, the three key features of Amazon Bedrock seem to be:\n",
      "\n",
      "1. Model choice and flexibility - Amazon Bedrock offers a variety of foundation models (FMs) from different providers, allowing customers to choose the models that work best for their use cases.\n",
      "\n",
      "2. Customization capabilities - Customers can customize the foundation models using their own private data through techniques like fine-tuning and RAIL (Rapid Adaptation with Integrated Learning). This allows them to build more tailored generative AI applications.\n",
      "\n",
      "3. Security and governance - Amazon Bedrock has security and privacy built-in and offers new governance capabilities like integration with CloudWatch and CloudTrail. This allows customers to use Bedrock securely and monitor usage and activity.\n"
     ]
    }
   ],
   "source": [
    "question = \"what are the three key features of amazon bedrock?\"\n",
    "\n",
    "result = conv_chain({\"question\": question})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83779167-2f41-4460-920f-c1a92c547577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what are the three key features of amazon bedrock?'),\n",
       " AIMessage(content=' Based on the context provided, the three key features of Amazon Bedrock seem to be:\\n\\n1. Model choice and flexibility - Amazon Bedrock offers a variety of foundation models (FMs) from different providers, allowing customers to choose the models that work best for their use cases.\\n\\n2. Customization capabilities - Customers can customize the foundation models using their own private data through techniques like fine-tuning and RAIL (Rapid Adaptation with Integrated Learning). This allows them to build more tailored generative AI applications.\\n\\n3. Security and governance - Amazon Bedrock has security and privacy built-in and offers new governance capabilities like integration with CloudWatch and CloudTrail. This allows customers to use Bedrock securely and monitor usage and activity.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66368f3d",
   "metadata": {},
   "source": [
    "Now we ask another question relying on the memory of the previous question(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ee90647-511e-417d-ac52-86b34ecaa827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context provided, some of the key security and governance capabilities of Amazon Bedrock include:\n",
      "\n",
      "- Amazon Bedrock is now a HIPAA eligible service and can be used in compliance with GDPR, allowing it to be used in regulated industries like healthcare and finance.\n",
      "\n",
      "- It has new governance capabilities including integration with Amazon CloudWatch to track usage metrics and build customized dashboards. \n",
      "\n",
      "- It also has integration with AWS CloudTrail to monitor API activity and troubleshoot issues. \n",
      "\n",
      "- Security and privacy are built into Bedrock since day one. Customers' data remains encrypted at rest and in transit. \n",
      "\n",
      "- It has the same AWS access controls as other AWS services. \n",
      "\n",
      "- None of the customer's data is used to train the original foundation models. \n",
      "\n",
      "- Data remains secure and private when customizing foundation models.\n",
      "\n",
      "So in summary, Amazon Bedrock has robust security and governance capabilities to ensure customer data privacy, monitor usage, comply with regulations, and give customers control over their data.\n"
     ]
    }
   ],
   "source": [
    "question = \"Elaborate the third feature more.\"\n",
    "\n",
    "result = conv_chain({\"question\": question})\n",
    "\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8cf0e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what are the three key features of amazon bedrock?'),\n",
       " AIMessage(content=' Based on the context provided, the three key features of Amazon Bedrock seem to be:\\n\\n1. Model choice and flexibility - Amazon Bedrock offers a variety of foundation models (FMs) from different providers, allowing customers to choose the models that work best for their use cases.\\n\\n2. Customization capabilities - Customers can customize the foundation models using their own private data through techniques like fine-tuning and RAIL (Rapid Adaptation with Integrated Learning). This allows them to build more tailored generative AI applications.\\n\\n3. Security and governance - Amazon Bedrock has security and privacy built-in and offers new governance capabilities like integration with CloudWatch and CloudTrail. This allows customers to use Bedrock securely and monitor usage and activity.'),\n",
       " HumanMessage(content='Elaborate the third feature more.'),\n",
       " AIMessage(content=\" Based on the context provided, some of the key security and governance capabilities of Amazon Bedrock include:\\n\\n- Amazon Bedrock is now a HIPAA eligible service and can be used in compliance with GDPR, allowing it to be used in regulated industries like healthcare and finance.\\n\\n- It has new governance capabilities including integration with Amazon CloudWatch to track usage metrics and build customized dashboards. \\n\\n- It also has integration with AWS CloudTrail to monitor API activity and troubleshoot issues. \\n\\n- Security and privacy are built into Bedrock since day one. Customers' data remains encrypted at rest and in transit. \\n\\n- It has the same AWS access controls as other AWS services. \\n\\n- None of the customer's data is used to train the original foundation models. \\n\\n- Data remains secure and private when customizing foundation models.\\n\\nSo in summary, Amazon Bedrock has robust security and governance capabilities to ensure customer data privacy, monitor usage, comply with regulations, and give customers control over their data.\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
